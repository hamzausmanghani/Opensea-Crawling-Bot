{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import undetected_chromedriver as uc\n",
    "import pandas as pd\n",
    "import clipboard as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.makedirs(name)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def fetch_profile_links(file, listing_url, scroll_times, save=True, verbose=True):\n",
    "    urls = []\n",
    "    df = pd.read_csv(file)\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(listing_url)\n",
    "    profile_class = \"styles__StyledLink-sc-l6elh8-0 ikuMIO Blockreact__Block-sc-1xf18x6-0 kdnPIp AccountLink--ellipsis-overflow\"\n",
    "    for i in range(scroll_times):\n",
    "        driver.execute_script(\"window.scrollBy(0, 1000)\")\n",
    "        page_source = driver.page_source\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(page_source)\n",
    "        user_tags = soup.find_all(\"a\", {\"class\": profile_class})\n",
    "        urls.extend([f\"{domain}/{i.get('href')}\" for i in user_tags])\n",
    "    new_urls = set(urls) - set(df.url.to_list())\n",
    "    new_data = {\"url\": list(new_urls), \"processed\": [\"no\"]*len(new_urls)}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    new_df = pd.concat([df, new_df]).reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print(f\"Total new unique fetched Urls: {len(set(urls))}\\n\",\n",
    "              \"Processing Summary:\\n\",\n",
    "              f\"\\tTotal Urls: {len(new_df)}\\n\",\n",
    "             f\"\\tProcessed Urls: {len(new_df[new_df.processed == 'yes'])}\\n\",\n",
    "             f\"\\tUnprocessed Urls: {len(new_df[new_df.processed == 'no'])}\\n\")\n",
    "    if save:\n",
    "        new_df.to_csv(file, index=False)\n",
    "    driver.close()\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def fetch_profile_detail(driver, url, req_social):\n",
    "    entities = []\n",
    "    detail = {}\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    detail['profile_url'] = url\n",
    "    detail['name'] = soup.find(\"div\", {\"class\": \"Overflowreact__OverflowContainer-sc-7qr9y8-0 jPSCbX AccountHeader--title\"}).text\n",
    "    info = soup.find_all(\"li\", {\"class\": \"Menureact__StyledListMenuItem-sc-1j0z9gq-3 hlufrI\"})\n",
    "    social_media_tag = soup.find_all(\"a\", {\"class\": \"styles__StyledLink-sc-l6elh8-0 ekTmzq Blockreact__Block-sc-1xf18x6-0 Buttonreact__StyledButton-sc-glfma3-0 kXZare kdWcfm ButtonGroupreact__StyledButton-sc-1skvztv-0 eztnHW AccountLinksBar--icon-button\"})\n",
    "    driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div[1]/div[3]/div[3]/div/button').click() # To get address in clipboard\n",
    "    time.sleep(1)\n",
    "    detail[\"address\"] = c.paste()\n",
    "    \n",
    "    if len(info) > 3:\n",
    "        info = info[:3]\n",
    "    for i in info:\n",
    "        span = i.find_all('span')\n",
    "        for s in span:\n",
    "            entity = s.text\n",
    "            if entity is not None:\n",
    "                entities.append(entity)\n",
    "        if len(span) < 2:\n",
    "            entities.append(\"0\")\n",
    "    entities = dict(zip(entities[::2], entities[1::2]))\n",
    "    detail.update(entities)\n",
    "                    \n",
    "    for tag in social_media_tag:\n",
    "        url = tag.get(\"href\")\n",
    "        if url is not None:\n",
    "            url_spl = re.split(\"/|.com\", url)\n",
    "            if len(url_spl) >= 2:\n",
    "                domain = url_spl[2]\n",
    "                if domain in req_social:\n",
    "                    detail[domain] = url\n",
    "                    \n",
    "    return detail\n",
    "\n",
    "\n",
    "def iterator(folder, url_file, complete_data_file, n_process, req_social, save=True, verbose=False):\n",
    "    url_df = pd.read_csv(url_file)\n",
    "    data = []\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    count = 0\n",
    "    for i, v in url_df.iterrows():\n",
    "        if (count < n_process) and (v.processed == \"no\"):\n",
    "            count += 1 \n",
    "            url = v.url\n",
    "            if verbose: print(count, \"/\", n_process)\n",
    "            try:\n",
    "                detail = fetch_profile_detail(driver, url, req_social)\n",
    "                data.append(detail)\n",
    "                url_df.loc[i, \"processed\"] = \"yes\"  \n",
    "            except:\n",
    "                if verbose: print(f\"Error at url {i} i-e, {url}\")  \n",
    "    df = pd.DataFrame(data)\n",
    "    if save:\n",
    "        current_datetime = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        url_df.to_csv(url_file, index=False)\n",
    "        create_folder(folder)\n",
    "        # current extraction\n",
    "        df.to_csv(f\"{folder}/Scrapped-Data-{current_datetime}.csv\", index=False)\n",
    "        # merge\n",
    "        complete_df = pd.read_csv(f\"{folder}/{complete_data_file}\")\n",
    "        complete_df = pd.concat([complete_df, df]).reset_index(drop=True)\n",
    "        complete_df.to_csv(f\"{folder}/{complete_data_file}\", index=False)\n",
    "    if verbose:\n",
    "        print(\"Processing Summary:\\n\",\n",
    "              f\"\\tTotal Urls: {len(url_df)}\\n\",\n",
    "             f\"\\tProcessed Urls: {len(url_df[url_df.processed == 'yes'])}\\n\",\n",
    "             f\"\\tUnprocessed Urls: {len(url_df[url_df.processed == 'no'])}\\n\")\n",
    "    driver.close()\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://opensea.io'\n",
    "listing_url = domain + \"//\" + \"activity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract unique profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"Scrapped-Profiles.csv\"\n",
    "urls_df = fetch_profile_links(file, listing_url, scroll_times=4, save=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_social = [\"twitter\", \"instagram\"]\n",
    "folder = \"scrapped_data\"\n",
    "url_file = \"Scrapped-Profiles.csv\"\n",
    "complete_data_file = \"Complete-Data.csv\"\n",
    "n_process = 10\n",
    "df = iterator(folder, url_file, complete_data_file, n_process, req_social, save=True, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
