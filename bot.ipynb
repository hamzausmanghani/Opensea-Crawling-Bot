{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install undetected-chromedriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import undetected_chromedriver as uc\n",
    "import pandas as pd\n",
    "import clipboard as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_profile_links(file, listing_url, scroll_times, save=True, verbose=True):\n",
    "    urls = []\n",
    "    df = pd.read_csv(file)\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    driver.get(listing_url)\n",
    "    profile_class = \"styles__StyledLink-sc-l6elh8-0 ikuMIO Blockreact__Block-sc-1xf18x6-0 kdnPIp AccountLink--ellipsis-overflow\"\n",
    "    for i in range(scroll_times):\n",
    "        driver.execute_script(\"window.scrollBy(0, 1000)\")\n",
    "        page_source = driver.page_source\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(page_source)\n",
    "        user_tags = soup.find_all(\"a\", {\"class\": profile_class})\n",
    "        urls.extend([f\"{domain}/{i.get('href')}\" for i in user_tags])\n",
    "    new_urls = set(urls) - set(df.url.to_list())\n",
    "    new_data = {\"url\": list(new_urls), \"processed\": [\"no\"]*len(new_urls)}\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    new_df = pd.concat([df, new_df]).reset_index(drop=True)\n",
    "    if verbose:\n",
    "        print(f\"Total new unique fetched Urls: {len(set(urls))}\\n\",\n",
    "              \"Processing Summary:\\n\",\n",
    "              f\"\\tTotal Urls: {len(new_df)}\\n\",\n",
    "             f\"\\tProcessed Urls: {len(new_df[new_df.processed == 'yes'])}\\n\",\n",
    "             f\"\\tUnprocessed Urls: {len(new_df[new_df.processed == 'no'])}\\n\")\n",
    "    if save:\n",
    "        new_df.to_csv(file, index=False)\n",
    "    driver.close()\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def fetch_profile_detail(driver, url, req_social):\n",
    "    entities = []\n",
    "    detail = {}\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source)\n",
    "    detail['profile_url'] = url\n",
    "    detail['name'] = soup.find(\"div\", {\"class\": \"Overflowreact__OverflowContainer-sc-7qr9y8-0 jPSCbX AccountHeader--title\"}).text\n",
    "    info = soup.find_all(\"li\", {\"class\": \"Menureact__StyledListMenuItem-sc-1j0z9gq-3 hlufrI\"})\n",
    "    social_media_tag = soup.find_all(\"a\", {\"class\": \"styles__StyledLink-sc-l6elh8-0 ekTmzq Blockreact__Block-sc-1xf18x6-0 Buttonreact__StyledButton-sc-glfma3-0 kXZare kdWcfm ButtonGroupreact__StyledButton-sc-1skvztv-0 eztnHW AccountLinksBar--icon-button\"})\n",
    "    driver.find_element_by_xpath('//*[@id=\"main\"]/div/div/div[1]/div[3]/div[3]/div/button').click() # To get address in clipboard\n",
    "    time.sleep(1)\n",
    "    detail[\"address\"] = c.paste()\n",
    "    \n",
    "    if len(info) > 3:\n",
    "        info = info[:3]\n",
    "    for i in info:\n",
    "        span = i.find_all('span')\n",
    "        for s in span:\n",
    "            entity = s.text\n",
    "            if entity is not None:\n",
    "                entities.append(entity)\n",
    "        if len(span) < 2:\n",
    "            entities.append(\"0\")\n",
    "    entities = dict(zip(entities[::2], entities[1::2]))\n",
    "    detail.update(entities)\n",
    "                    \n",
    "    for tag in social_media_tag:\n",
    "        url = tag.get(\"href\")\n",
    "        if url is not None:\n",
    "            url_spl = re.split(\"/|.com\", url)\n",
    "            if len(url_spl) >= 2:\n",
    "                domain = url_spl[2]\n",
    "                if domain in req_social:\n",
    "                    detail[domain] = url\n",
    "                    \n",
    "    return detail\n",
    "\n",
    "\n",
    "def iterator(url_file, n_process, req_social, save=True, verbose=False):\n",
    "    url_df = pd.read_csv(url_file)\n",
    "    data = []\n",
    "    driver = uc.Chrome(use_subprocess=True)\n",
    "    count = 0\n",
    "    try:\n",
    "        for i, v in url_df.iterrows():\n",
    "            if (count < n_process) and (v.processed == \"no\"):\n",
    "                count += 1 \n",
    "                url = v.url\n",
    "                if verbose: print(count, \"/\", n_process)\n",
    "                detail = fetch_profile_detail(driver, url, req_social)\n",
    "                data.append(detail)\n",
    "                url_df.loc[i, \"processed\"] = \"yes\"    \n",
    "    except:\n",
    "        if verbose: print(f\"Error at url {i} i-e, {url}\")\n",
    "    df = pd.DataFrame(data)\n",
    "    if save:\n",
    "        current_datetime = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        url_df.to_csv(file, index=False)\n",
    "        df.to_csv(f\"Scrapped-Data-{current_datetime}.csv\", index=False)\n",
    "    if verbose:\n",
    "        print(\"Processing Summary:\\n\",\n",
    "              f\"\\tTotal Urls: {len(url_df)}\\n\",\n",
    "             f\"\\tProcessed Urls: {len(url_df[url_df.processed == 'yes'])}\\n\",\n",
    "             f\"\\tUnprocessed Urls: {len(url_df[url_df.processed == 'no'])}\\n\")\n",
    "#     driver.close()\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://opensea.io'\n",
    "listing_url = domain + \"//\" + \"activity\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract unique profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total new unique fetched Urls: 45\n",
      " Processing Summary:\n",
      " \tTotal Urls: 343\n",
      " \tProcessed Urls: 3\n",
      " \tUnprocessed Urls: 340\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"Scrapped-Profiles.csv\"\n",
    "urls_df = fetch_profile_links(file, listing_url, scroll_times=30, save=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza.usman\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:40: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 4\n",
      "3 / 4\n",
      "4 / 4\n",
      "Processing Summary:\n",
      " \tTotal Urls: 343\n",
      " \tProcessed Urls: 7\n",
      " \tUnprocessed Urls: 336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "req_social = [\"twitter\", \"instagram\"]\n",
    "url_file = \"Scrapped-Profiles.csv\"\n",
    "n_process = 4\n",
    "df = iterator(url_file, n_process, req_social, save=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_url</th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>Collected</th>\n",
       "      <th>Created</th>\n",
       "      <th>Favorited</th>\n",
       "      <th>twitter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://opensea.io//0xDD66C65f4aB47fD904850b93...</td>\n",
       "      <td>Unnamed</td>\n",
       "      <td>0xDD66C65f4aB47fD904850b9342236aDed3813Ec3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://opensea.io//0x2461</td>\n",
       "      <td>0x2461</td>\n",
       "      <td>verbose=True</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://opensea.io//Mrjosco_Vault</td>\n",
       "      <td>Mrjosco_Vault</td>\n",
       "      <td>0xA90E6Ff084580B1194f7333a702De7641E33DFb9</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>https://twitter.com/@mrjosco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://opensea.io//0x8DCeeb78462b002d71526ad0...</td>\n",
       "      <td>Unnamed</td>\n",
       "      <td>0x8DCeeb78462b002d71526ad0CeFB68bC3B001367</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         profile_url           name  \\\n",
       "0  https://opensea.io//0xDD66C65f4aB47fD904850b93...        Unnamed   \n",
       "1                         https://opensea.io//0x2461         0x2461   \n",
       "2                  https://opensea.io//Mrjosco_Vault  Mrjosco_Vault   \n",
       "3  https://opensea.io//0x8DCeeb78462b002d71526ad0...        Unnamed   \n",
       "\n",
       "                                      address Collected Created Favorited  \\\n",
       "0  0xDD66C65f4aB47fD904850b9342236aDed3813Ec3        13       0         0   \n",
       "1                                verbose=True         4       0        35   \n",
       "2  0xA90E6Ff084580B1194f7333a702De7641E33DFb9        18       0         2   \n",
       "3  0x8DCeeb78462b002d71526ad0CeFB68bC3B001367        13       0         0   \n",
       "\n",
       "                        twitter  \n",
       "0                           NaN  \n",
       "1                           NaN  \n",
       "2  https://twitter.com/@mrjosco  \n",
       "3                           NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
